#!/bin/bash

set -euo pipefail

# finalize workload cluster(s) bootstrap or create a new workload cluster.
# run `./<repo_root>/scripts/workload-cluster.sh -h` to learn more.

REPO_ROOT=$(dirname "${BASH_SOURCE[0]}")/..
tempdir=$(mktemp -d)

# Management cluster kube config and context
MGMT_CTX=""
MGMT_CFG="" # default $HOME/.kube/config

# cluster config file containing all settings required for
# spinning up a new cluster
CONFIG_FILE=""

# kubectl configured to talk to management cluster, based on use input
KUBECTL_MGMT=""

# kubectl configured to talk to a workload cluster, based on discovery
KUBECTL=""

main() {

while [[ $# -gt 0 ]]; do
  case $1 in
    -m|--management-cluster-context)
      MGMT_CTX=$2; shift
      ;;
    -k|--management-cluster-kubeconfig)
      MGMT_CFG=$2; shift
      ;;
    -f|--config-file)
      CONFIG_FILE=$2; shift
      ;;
    -h|--help)
      show_help
      ;;
    *)
      show_help
      ;;
  esac
  shift
done

set +x
. $REPO_ROOT/config/shared.env
set -x

if [ -z "$MGMT_CTX" ]; then
  echo Management cluster context is required # && exit 1
fi
if [ -z "$MGMT_CFG" ]; then
  echo Management cluster kubeconfing not provided, $HOME/.kube/config is assumed
  MGMT_CFG="$HOME/.kube/config"
  KUBECTL_MGMT="kubectl --kubeconfig $MGMT_CFG --context $MGMT_CTX"
fi
if [ -z "$CONFIG_FILE" ]; then
  # config file must be $REPO_ROOT/config/<cluster>.env
  echo Cluster config file not provided - will finalize existing workload clusters.
  finalize
else
  echo Create cluster from $CONFIG_FILE
  create
fi

}

create() {
  # config file must be created manually before using this script
  # these settings should not be autogenerated, it's up to the user to configure
  set +x
  . $CONFIG_FILE
  set -x

  cluster_dir=$REPO_ROOT/infrastructure/control-plane-cluster/$CLUSTER_NAME
  # if directory already exists, then this can be used as a way to upgrade contents
  mkdir -p $cluster_dir

  envsubst < $REPO_ROOT/templates/capi-workload-kustomization.yaml > $cluster_dir/kustomization.yaml
  envsubst < $REPO_ROOT/templates/capi-workload-namespace.yaml > $cluster_dir/namespace.yaml
  envsubst < $REPO_ROOT/templates/aws/cluster.yaml > $cluster_dir/cluster.yaml

  # I don't want to give flux deploy key with write permissions, therefore 'bootstrap' is not an option
  # 'flux install --export' does not have options to generate gotk-sync.yaml, so instead this will be
  # instantiated from template
  # This is only needed when adding a cluster for the first time to the repo. On the following invocations, flux is deployed as CRS
  flux_crs=$tempdir/flux-combined.yaml
  dest_dir=$REPO_ROOT/clusters/staging/${CLUSTER_NAME}/flux-system
  mkdir -p $dest_dir
  flux install --version=$FLUXCD_VERSION --export > $dest_dir/gotk-components.yaml
  envsubst < $REPO_ROOT/templates/gotk-sync.yaml > $dest_dir/gotk-sync.yaml
  cp $dest_dir/gotk-components.yaml $flux_crs
  echo "---" >> $flux_crs
  cat $dest_dir/gotk-sync.yaml >> $flux_crs

  # now we can put this in CM. (k create cm accepts --from-<whatever> multiple times,
  # but it creates a separate data entry for each occurence, that's why concatenating file was necessary
  kubectl create configmap crs-cm-flux-${FLUXCD_VERSION} --from-file=$flux_crs -n $CLUSTER_NAME --dry-run=client -o yaml > $cluster_dir/crs-cm-flux-${FLUXCD_VERSION}.yaml

  # TODO - add new cluster dir to kustomization.yaml
  # $REPO_ROOT/infrastructure/control-plane-cluster/kustomization.yaml

}

finalize_cluster() {
  local cluster=$1
  local ns=$2
  echo Finalizing cluster $cluster in $ns namespace

  while ! $KUBECTL_MGMT wait --context mgmt --for condition=ResourcesApplied=True clusterresourceset crs -n cluster-dev --timeout=15s ; do
    echo $(date '+%F %H:%M:%S') waiting for workload cluster to become ready
    sleep 15
  done

  kubeconfig=$REPO_ROOT/$cluster.kubeconfig
  clusterctl --kubeconfig=$MGMT_CFG --kubeconfig-context $MGMT_CTX get kubeconfig $cluster -n $ns > $kubeconfig
  chmod go-r $kubeconfig

  set +e
  echo $(date '+%F %H:%M:%S') - Waiting for workload cluster to become responsive
  while [ -z $($KUBECTL_WORKLOAD get pod -n kube-system -l component=kube-apiserver -o name) ]; do sleep 10; done
  set -e

  kas=$($KUBECTL_WORKLOAD get pod -n kube-system -l component=kube-apiserver -o name)
  export K8S_SERVICE_HOST=$($KUBECTL_WORKLOAD get $kas -n kube-system --template '{{.status.podIP}}')
  export K8S_SERVICE_PORT='6443'

  set +x
  . $REPO_ROOT/config/$cluster.env
  set -x

  # envsubst in heml values.yaml: https://github.com/helm/helm/issues/10026
  envsubst < ${REPO_ROOT}/templates/cni/cilium-values-${CILIUM_VERSION}.yaml | \
    helm install cilium cilium/cilium --version $CILIUM_VERSION \
    --kubeconfig $kubeconfig \
    --namespace kube-system -f -

  # on clusters that already existed in the git repo before deploying
  # flux is installed via CRS, so no need to do it in script
  kubectl --kubeconfig=$kubeconfig create secret generic flux-system -n flux-system \
    --from-file identity=$FLUX_KEY_PATH  \
    --from-file identity.pub=$FLUX_KEY_PATH.pub \
    --from-literal known_hosts="$GITHUB_KNOWN_HOSTS"
}

# Discover workload clusters and complete setup if required.
finalize() {
  clusters=$($KUBECTL_MGMT get clusters -A --no-headers=true)
  # example line:
  # cluster-01    dev    Provisioned   50m
  for line in $clusters; do
    cluster=$(echo $line | awk '{print $1}')
    ns=$(echo $line | awk '{print $2}')
    if :; then # if required then
      finalize_cluster $cluster $ns
    fi
  done
}

show_help() {
  echo Bootstrap a new workload cluster or finalise
  echo installation of existing CAPI workload clusters
  echo Usage:
  echo "-k|--management-cluster-kubeconfig - optional, management cluster kubeconfig, default $HOME/.kube/config"
  echo "-m|--management-cluster-context - optional, management cluster kubeconfig context"
  echo "-f|--config-file - optional, if provided this must be a config file in $REPO_ROOT/config. In this case"
  echo "  the script will generate payload for the cluster and commit it to the repo, from where"
  echo "  it will be synced by flux on the management cluster"
  exit 0
}

main "$@"
