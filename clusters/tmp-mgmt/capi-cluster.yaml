---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: "cluster-mgmt"
  namespace: "cluster-mgmt"
  labels:
    cluster.x-k8s.io/cluster-name: "cluster-mgmt"
    cluster-role: mgmt
    cilium-mesh: none
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - "192.168.0.0/20"
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: AWSCluster
    name: "cluster-mgmt"
  controlPlaneRef:
    kind: KubeadmControlPlane
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    name: "cluster-mgmt-control-plane"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSCluster
metadata:
  name: "cluster-mgmt"
  namespace: "cluster-mgmt"
spec:
  region: "ap-southeast-2"
  sshKeyName: "aws"
  network:
    vpc:
      availabilityZoneUsageLimit: 1
    cni:
      cniIngressRules:
      - description: "(cilium) VXLAN overlay"
        protocol: udp
        fromPort: 8472
        toPort: 8472
      - description: "(cilium) health checks"
        protocol: tcp
        fromPort: 4240
        toPort: 4240
---
kind: KubeadmControlPlane
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
metadata:
  name: "cluster-mgmt-control-plane"
  namespace: "cluster-mgmt"
spec:
  replicas: 1
  machineTemplate:
    infrastructureRef:
      kind: AWSMachineTemplate
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      name: "cluster-mgmt-control-plane"
  kubeadmConfigSpec:
    initConfiguration:
      nodeRegistration:
        name: '{{ ds.meta_data.local_hostname }}'
        kubeletExtraArgs:
          cloud-provider: aws
      skipPhases:
      - addon/kube-proxy
    clusterConfiguration:
      apiServer:
        extraArgs:
          cloud-provider: aws
      controllerManager:
        extraArgs:
          cloud-provider: aws
    joinConfiguration:
      nodeRegistration:
        name: '{{ ds.meta_data.local_hostname }}'
        kubeletExtraArgs:
          cloud-provider: aws
  version: "1.25.4"
---
kind: AWSMachineTemplate
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
metadata:
  name: "cluster-mgmt-control-plane"
  namespace: "cluster-mgmt"
spec:
  template:
    spec:
      instanceType: "t3.medium"
      iamInstanceProfile: "control-plane.cluster-api-provider-aws.sigs.k8s.io"
      sshKeyName: "aws"
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: "cluster-mgmt-md-0"
  namespace: "cluster-mgmt"
spec:
  clusterName: "cluster-mgmt"
  replicas: 1
  selector:
    matchLabels:
  template:
    spec:
      clusterName: "cluster-mgmt"
      version: "1.25.4"
      bootstrap:
        configRef:
          name: "cluster-mgmt-md-0"
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
      infrastructureRef:
        name: "cluster-mgmt-md-0"
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: AWSMachineTemplate
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSMachineTemplate
metadata:
  name: "cluster-mgmt-md-0"
  namespace: "cluster-mgmt"
spec:
  template:
    spec:
      instanceType: "t3.medium"
      iamInstanceProfile: "nodes.cluster-api-provider-aws.sigs.k8s.io"
      sshKeyName: "aws"
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: "cluster-mgmt-md-0"
  namespace: "cluster-mgmt"
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          name: '{{ ds.meta_data.local_hostname }}'
          kubeletExtraArgs:
            cloud-provider: aws
            max-pods: '64'
